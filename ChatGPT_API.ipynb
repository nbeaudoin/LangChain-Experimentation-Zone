{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOwocBlJcC+95YwS+Dgh3jr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nbeaudoin/LangChain-Experimentation-Zone/blob/main/ChatGPT_API.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tutorial Source: https://www.youtube.com/watch?v=phHqvLHCwH4&list=PL8motc6AQftk1Bs42EW45kwYbyJ4jOdiZ&index=3"
      ],
      "metadata": {
        "id": "Hg3N6mwaOVse"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_QHRcSvdN4vd",
        "outputId": "ab7eea86-1697-42c7-9679-b91dc679d17e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "!pip -q install openai python-dotenv langchain==0.0.99rc0\n",
        "\n",
        "import os\n",
        "import openai\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Load variables from .env file\n",
        "load_dotenv('/content/drive/MyDrive/Projects/keys/secrets.json')\n",
        "\n",
        "# Use variables\n",
        "openai_api_key = os.getenv('OPENAI_API_KEY')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = openai.ChatCompletion.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Hello what kind of assistant are you?\"}\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "9Ywmqo4qQfLI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JeCh8kCJZv1R",
        "outputId": "5ea9edac-1f34-4f81-85a0-4950c8f4f107"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<OpenAIObject chat.completion id=chatcmpl-88vpceMmbjIK9bDbFGnGdEn3oPNcR at 0x7f7c25f6f7e0> JSON: {\n",
              "  \"id\": \"chatcmpl-88vpceMmbjIK9bDbFGnGdEn3oPNcR\",\n",
              "  \"object\": \"chat.completion\",\n",
              "  \"created\": 1697139620,\n",
              "  \"model\": \"gpt-3.5-turbo-0613\",\n",
              "  \"choices\": [\n",
              "    {\n",
              "      \"index\": 0,\n",
              "      \"message\": {\n",
              "        \"role\": \"assistant\",\n",
              "        \"content\": \"I am an AI assistant designed to provide information, answer questions, and assist with tasks to the best of my abilities. How can I help you today?\"\n",
              "      },\n",
              "      \"finish_reason\": \"stop\"\n",
              "    }\n",
              "  ],\n",
              "  \"usage\": {\n",
              "    \"prompt_tokens\": 25,\n",
              "    \"completion_tokens\": 31,\n",
              "    \"total_tokens\": 56\n",
              "  }\n",
              "}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant named Kate.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Hello what kind of assistant are you?\"}\n",
        "]"
      ],
      "metadata": {
        "id": "eI5wiaV0Z4_9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conversation_total_tokens = 0\n",
        "\n",
        "while True:\n",
        "  message = input(\"Human: \")\n",
        "  if message=='exit':\n",
        "    print(f\"{conversation_total_tokens} tokens used in total in this conversation\")\n",
        "    break\n",
        "  if message:\n",
        "    messages.append(\n",
        "        {\"role\": \"user\", \"content\": message},\n",
        "    )\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\", messages=messages\n",
        "    )\n",
        "  reply = response.choices[0].message.content\n",
        "  total_tokens = response.usage[\"total_tokens\"]\n",
        "  conversation_total_tokens += total_tokens\n",
        "  print(f\"ChatGPT: {reply} \\n {total_tokens} tokens used\")\n",
        "  messages.append({\"role\": \"assistant\", \"content\": reply})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l5sumvdWbe8W",
        "outputId": "3231c094-ed45-4ae9-fe74-a0eef7ad0a1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Human: Who was the last President of the USA?\n",
            "ChatGPT: The Los Angeles Dodgers won the World Series in 2020. And the last President of the United States is Joe Biden. \n",
            " 81 tokens used\n",
            "Human: Who was the president before that?\n",
            "ChatGPT: The President before Joe Biden was Donald Trump. \n",
            " 105 tokens used\n",
            "Human: exit\n",
            "186 tokens used in total in this conversation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show langchain"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P4NaR_I4cWW7",
        "outputId": "6f452eb6-34c8-4afb-d830-6a084038d85f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: langchain\n",
            "Version: 0.0.99rc0\n",
            "Summary: Building applications with LLMs through composability\n",
            "Home-page: https://www.github.com/hwchase17/langchain\n",
            "Author: \n",
            "Author-email: \n",
            "License: MIT\n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: aiohttp, aleph-alpha-client, dataclasses-json, deeplake, numpy, pydantic, PyYAML, requests, SQLAlchemy, tenacity\n",
            "Required-by: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import PromptTemplate, LLMChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.llms import OpenAI, OpenAIChat"
      ],
      "metadata": {
        "id": "AkDsZd0Fd3Xx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prefix_messages = [{\"role\": \"system\", \"content\": \"You are a helpful history professor named Kate.\"}]"
      ],
      "metadata": {
        "id": "0gANaR3LeG9m"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = OpenAIChat(model_name='gpt-3.5-turbo',\n",
        "                 temperature=0,\n",
        "                 prefix_messages=prefix_messages,\n",
        "                 max_tokens=256)"
      ],
      "metadata": {
        "id": "4ZFh3M-anuj8"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"Take the following question: {user_input}\n",
        "\n",
        "Answer it in an informative and interesting but concise way for someone who is new to this topic.\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(template=template,\n",
        "                        input_variables=[\"user_input\"])"
      ],
      "metadata": {
        "id": "Hn7eQg_Kn5pz"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
        "\n",
        "user_input = \"When was Marcus Aurelius the emperior in Rome?\"\n",
        "\n",
        "llm_chain.run(user_input)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "gaUAl_dpoB3a",
        "outputId": "75da7370-1e1b-4c9f-ed6c-205da04dcf3c"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Marcus Aurelius was the emperor of Rome from 161 to 180 AD. He was a philosopher-king known for his stoic beliefs and his leadership during a challenging period in Roman history. His reign marked the end of the Pax Romana, a time of relative peace and stability in the empire. Despite facing numerous military conflicts and internal challenges, Marcus Aurelius is remembered for his wisdom and his writings, particularly his book \"Meditations,\" which offers insights into his personal reflections on life and leadership.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
        "\n",
        "user_input = \"Who was Marcus Aurelius married to?\"\n",
        "\n",
        "llm_chain.run(user_input)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "p_FnPoJOoTS6",
        "outputId": "1e656417-6e37-40c4-b365-ebb5b06ad243"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Marcus Aurelius, the Roman Emperor from 161 to 180 AD, was married to Faustina the Younger. Faustina was the daughter of Antoninus Pius, who preceded Marcus Aurelius as Emperor. Their marriage was arranged for political reasons, as was common among Roman aristocracy. Despite the arranged nature of their union, Marcus Aurelius and Faustina had a strong and loving relationship. They had thirteen children together, although sadly, only a few survived into adulthood. Faustina played an influential role in Marcus Aurelius' life and was highly regarded by the Roman people. After her death, Marcus Aurelius deified her, and she became known as Faustina the Divine.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "r6nak6HJpGW8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}