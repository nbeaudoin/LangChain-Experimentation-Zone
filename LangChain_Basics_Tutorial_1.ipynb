{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1D57mwaMWZdCQKSwbZdy3c2Q3lL6CWC75",
      "authorship_tag": "ABX9TyPqJRQhcaZIRNHsNCZnpvLd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nbeaudoin/LangChain-Experimentation-Zone/blob/main/LangChain_Basics_Tutorial_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load dependencies"
      ],
      "metadata": {
        "id": "qFdHquGJiH3Y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ed2sBaQpc8K8",
        "outputId": "c939c5c8-ebb1-4d13-9ba2-87ad9ba35ac0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.5/76.5 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.8/294.8 kB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-dotenv\n",
            "  Downloading python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
            "Installing collected packages: python-dotenv\n",
            "Successfully installed python-dotenv-1.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip -q install openai langchain huggingface_hub\n",
        "!pip install python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: load from drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "owOstrCRhK0i",
        "outputId": "a6f9bbbd-6d91-4e61-e25b-1ad4efec8823"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load variables from .env file\n",
        "load_dotenv('/content/drive/MyDrive/secrets.json')\n",
        "\n",
        "# Use variables\n",
        "openai_api = os.getenv('OPENAI_API_KEY')\n",
        "huggingface_api = os.getenv('HUGGINGFACE_API_KEY')\n",
        "\n",
        "# Print variables\n",
        "# print('Open API Key:', openai_api)\n",
        "# print('HF API Key:', huggingface_api)\n"
      ],
      "metadata": {
        "id": "oFcMeLTddFQd"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plain Conditional Generation\n",
        "## First with OpenAI GPT 3"
      ],
      "metadata": {
        "id": "jRZpKC9EiM2a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI"
      ],
      "metadata": {
        "id": "llVL6v6FhKfW"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = OpenAI(\n",
        "    model_name=\"text-davinci-003\",\n",
        "    temperature=0.9,\n",
        "    max_tokens=100\n",
        ")\n"
      ],
      "metadata": {
        "id": "JVKIODGEiTUe"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"The quick brown fox jumps over the lazy dog\"\n",
        "\n",
        "print(llm(text))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S0ci-ugWiX7U",
        "outputId": "1ce76bc9-8ef1-429a-d518-eea181baf541"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "The quick brown fox jumped over the lazy dog.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Now with T5-Flan-XL\n"
      ],
      "metadata": {
        "id": "o8vo15qhipDC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import HuggingFaceHub"
      ],
      "metadata": {
        "id": "MTmi3zm7igCH"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_hf = HuggingFaceHub(\n",
        "    repo_id=\"google/flan-t5-xl\",\n",
        "    model_kwargs={\"temperature\":0.9},\n",
        "    huggingfacehub_api_token=huggingface_api\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b_cjIg1omoUB",
        "outputId": "e4f07d8a-17a5-426e-adaa-c82d3cda1ee0"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_deprecation.py:127: FutureWarning: '__init__' (from 'huggingface_hub.inference_api') is deprecated and will be removed from version '0.19.0'. `InferenceApi` client is deprecated in favor of the more feature-complete `InferenceClient`. Check out this guide to learn how to convert your script to use it: https://huggingface.co/docs/huggingface_hub/guides/inference#legacy-inferenceapi-client.\n",
            "  warnings.warn(warning_message, FutureWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Why did the chicken cross the road?\"\n",
        "\n",
        "#print(llm_hf(text))"
      ],
      "metadata": {
        "id": "x_yAlb_xm3Fl"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prompt Templates"
      ],
      "metadata": {
        "id": "Zkf0m7Z4oBHP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import prompt\n",
        "from langchain import PromptTemplate\n",
        "\n",
        "location_template = \"\"\"\n",
        "I want you to act as a naming consultant for new golf courses.\n",
        "\n",
        "Return a list of golf course names. Each name should be short, catchy and easy to rememember.\n",
        "\n",
        "What are some good names for a golf course that is in a {location_description}?\n",
        "\"\"\"\n",
        "\n",
        "prompt_template = PromptTemplate(\n",
        "    input_variables=[\"location_description\"],\n",
        "    template=location_template\n",
        ")\n"
      ],
      "metadata": {
        "id": "rOkpC-rzoDMN"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "description = \"a low-lying swam and wetland area with lots of wildlife\"\n",
        "description_02 = \"a high mountain pass characterized by sights of birds\"\n",
        "description_03 = \"an ocean-side landscape with crashing waves and sandy dunes\"\n",
        "\n",
        "# to see what the prompt will be like\n",
        "prompt_template.format(location_description=description)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "2L6CQRifqqII",
        "outputId": "37a0840d-68b9-4286-a9d9-bbc3f8e0f761"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nI want you to act as a naming consultant for new golf courses.\\n\\nReturn a list of golf course names. Each name should be short, catchy and easy to rememember.\\n\\nWhat are some good names for a golf course that is in a a low-lying swam and wetland area with lots of wildlife?\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# querying the model with the prompt template\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "chain = LLMChain(\n",
        "    llm=llm,\n",
        "    prompt=prompt_template,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "print(chain.run(description_02))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2xYUiHyEq7cI",
        "outputId": "b05b3239-46e2-404b-e1d2-731db2a6f698"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "I want you to act as a naming consultant for new golf courses.\n",
            "\n",
            "Return a list of golf course names. Each name should be short, catchy and easy to rememember.\n",
            "\n",
            "What are some good names for a golf course that is in a a high mountain pass characterized by sights of birds?\n",
            "\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "1. Avian Vista \n",
            "2. Breezy Pass \n",
            "3. Highflight Links \n",
            "4. Blue Mount Golf \n",
            "5. Feathered Pines \n",
            "6. Winged Valley \n",
            "7. Alpine Birdie Links \n",
            "8. Plummeting Pines \n",
            "9. Skyway Golf \n",
            "10. Eagle's Height \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Few Shot Learning"
      ],
      "metadata": {
        "id": "_jABd25HaKYc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import PromptTemplate, FewShotPromptTemplate"
      ],
      "metadata": {
        "id": "fa4AYabL-KWY"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# First create the list of few show examples\n",
        "examples = [\n",
        "    {\"word\": \"happy\", \"antonym\": \"sad\"},\n",
        "    {\"word\": \"tall\", \"antonym\": \"short\"}\n",
        "\n",
        "]"
      ],
      "metadata": {
        "id": "8VfaQG9paPts"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the template to format the examples we did previously\n",
        "example_formatter_template = \"\"\"\n",
        "Word: {word}\n",
        "Antonym: {antonym}\\n\n",
        "\"\"\"\n",
        "\n",
        "example_prompt = PromptTemplate(\n",
        "    input_variables=[\"word\", \"antonym\"],\n",
        "    template=example_formatter_template\n",
        ")"
      ],
      "metadata": {
        "id": "l6_E70WSahZ8"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Finally, we creat the 'FewShotPromptTemplate' object\n",
        "few_shot_prompt = FewShotPromptTemplate(\n",
        "    examples=examples,\n",
        "    example_prompt=example_prompt,\n",
        "    prefix=\"Give the antonym of every input\",\n",
        "    suffix=\"Word: {input}\\nAntonym:\",\n",
        "    input_variables=[\"input\"],\n",
        "    example_separator=\"\\n\",\n",
        ")\n"
      ],
      "metadata": {
        "id": "yUrCHIO9bPyl"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We can now generate a prompt using the 'format' method\n",
        "print(few_shot_prompt.format(input=\"big\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wvw7LPPpywUd",
        "outputId": "35884b15-70b5-46f7-a415-939cdf6916ea"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Give the antonym of every input\n",
            "\n",
            "Word: happy\n",
            "Antonym: sad\n",
            "\n",
            "\n",
            "\n",
            "Word: tall\n",
            "Antonym: short\n",
            "\n",
            "\n",
            "Word: big\n",
            "Antonym:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import LLMChain\n",
        "\n",
        "chain = LLMChain(\n",
        "    llm=llm,\n",
        "    prompt=few_shot_prompt,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# Run the chain only specifying the input variable\n",
        "print(chain.run(\"big\"))\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PHZu3Cis2K7q",
        "outputId": "ad1adce5-f69d-4f1c-826b-23fa5d441a82"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mGive the antonym of every input\n",
            "\n",
            "Word: happy\n",
            "Antonym: sad\n",
            "\n",
            "\n",
            "\n",
            "Word: tall\n",
            "Antonym: short\n",
            "\n",
            "\n",
            "Word: big\n",
            "Antonym:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            " small\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DG_4fDRC4E6i"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}